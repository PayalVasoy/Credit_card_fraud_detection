{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Pacakages\n",
    "\n",
    "import pandas as pd #Data Processing\n",
    "from sklearn.preprocessing import StandardScaler #Data Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the dataset\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.drop('Time', axis=1, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a look at the number of fraud and non-fraud cases in our dataset as well as  also calculate the percentage of fraud cases in the total number of recorded transactions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transactions is :  284807\n",
      "Noramal Transactions is :  284315\n",
      "Abnormal Transactions is :  492\n",
      "Total Fraud Percentage is :  0.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cases = len(df)\n",
    "nonfraud_count = len(df[df.Class == 0])\n",
    "fraud_count = len(df[df.Class == 1])\n",
    "fraud_percentage = round(fraud_count/nonfraud_count*100,2)\n",
    "\n",
    "print(\"Total Transactions is : \",cases)\n",
    "print(\"Noramal Transactions is : \",nonfraud_count)\n",
    "print(\"Abnormal Transactions is : \",fraud_count)\n",
    "print(\"Total Fraud Percentage is : \",fraud_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are just 492 fraud incidents out of 284,807 samples, or 0.17 percent of all samples. As a result, I may conclude that the data I am working with is very unbalanced and must be treated with caution while modelling and assessing it. Next, I will use Python's 'describe' function to gain a statistical perspective of both fraud and non-fraud transaction amount data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code can also be used to check for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the credit card dataset, look for null values. Fortunately, our dataset contains no null or NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Amount\" feature is the one I was most interested in. The following is a summary of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonfraud transaction statistics \n",
      "\n",
      "count    284315.000000\n",
      "mean         88.291022\n",
      "std         250.105092\n",
      "min           0.000000\n",
      "25%           5.650000\n",
      "50%          22.000000\n",
      "75%          77.050000\n",
      "max       25691.160000\n",
      "Name: Amount, dtype: float64 \n",
      "\n",
      "fraud transaction statistics \n",
      " \n",
      "count     492.000000\n",
      "mean      122.211321\n",
      "std       256.683288\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         9.250000\n",
      "75%       105.890000\n",
      "max      2125.870000\n",
      "Name: Amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "nonfraud_count = df[df.Class == 0]\n",
    "fraud_count = df[df.Class == 1]\n",
    "\n",
    "print(\"Nonfraud transaction statistics \\n\")\n",
    "print(nonfraud_count.Amount.describe(),\"\\n\")\n",
    "\n",
    "print(\"fraud transaction statistics \\n \")\n",
    "print(fraud_count.Amount.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the data, it's clear that the 'Amount' variable's values vary greatly when compared to the other variables. We may normalise it in Python using the 'StandardScaler' function to lessen its large range of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.244964\n",
      "1    -0.342475\n",
      "2     1.160686\n",
      "3     0.140534\n",
      "4    -0.073403\n",
      "5    -0.338556\n",
      "6    -0.333279\n",
      "7    -0.190107\n",
      "8     0.019392\n",
      "9    -0.338516\n",
      "10   -0.322044\n",
      "11   -0.313289\n",
      "12    0.132538\n",
      "13   -0.243282\n",
      "14   -0.118142\n",
      "15   -0.289300\n",
      "16   -0.301294\n",
      "17   -0.349671\n",
      "18   -0.166119\n",
      "19   -0.333239\n",
      "Name: Amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "amount = df['Amount'].values\n",
    "\n",
    "df['Amount'] = sc.fit_transform(amount.reshape(-1, 1))\n",
    "print(df['Amount'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
